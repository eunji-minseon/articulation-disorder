import os
import time
import numpy as np
import streamlit as st
import ast
from video.extract_mouth_landmarks import extract_mouth_landmarks

# ê²½ë¡œ ì„¤ì •
RAW_DIR = "data/raw"
PROCESSED_DIR = "data/processed"
os.makedirs(RAW_DIR, exist_ok=True)
os.makedirs(PROCESSED_DIR, exist_ok=True)

# ë¬¸ì¥ â†’ ê¸°ì¤€ ì¢Œí‘œ íŒŒì¼ prefix
sentence_to_file = {
    "ê°•ì•„ì§€ê°€ ì§–ê³  ìˆì–´ìš”": "normal1",
    "í† ë¼ê°€ í’€ì„ ë¨¹ì–´ìš”": "normal2",
    "ì½”ë¼ë¦¬ëŠ” ì½”ê°€ ê¸¸ì–´ìš”": "normal3",
    "ì°½ë¬¸ì„ ì—´ì–´ ì£¼ì„¸ìš”": "normal4",
    "ê³ ì–‘ì´ê°€ ì•¼ì˜¹í•´ìš”": "normal5",
    "ë¬´ì§€ê°œê°€ ë–´ì–´ìš”": "normal6",
    "ìë™ì°¨ê°€ ë‹¬ë ¤ìš”": "normal7",
    "ë³„ì´ ë°˜ì§ë°˜ì§ ë¹›ë‚˜ìš”": "normal8",
    "ë‚˜ëŠ” ì‚¬ê³¼ë¥¼ ì¢‹ì•„í•´ìš”": "normal9",
    "ì˜¤ëŠ˜ì€ ê¸°ë¶„ì´ ì¢‹ì•„ìš”": "normal10"
}

# ë¬¸ì¥ â†’ ë¶„ì„ ìŒì†Œ
sentence_analysis = {
    "ê°•ì•„ì§€ê°€ ì§–ê³  ìˆì–´ìš”": ["ã„±", "ã…‡", "ã…ˆ", "ã…†"],
    "í† ë¼ê°€ í’€ì„ ë¨¹ì–´ìš”": ["ã…Œ", "ã„²", "ã…", "ã„¹"],
    "ì½”ë¼ë¦¬ëŠ” ì½”ê°€ ê¸¸ì–´ìš”": ["ã…‹", "ã„²", "ã„¹"],
    "ì°½ë¬¸ì„ ì—´ì–´ ì£¼ì„¸ìš”": ["ã…Š", "ã…", "ã…ˆ", "ã……"],
    "ê³ ì–‘ì´ê°€ ì•¼ì˜¹í•´ìš”": ["ã„±", "ã…‡", "ã…‘", "ã…"],
    "ë¬´ì§€ê°œê°€ ë–´ì–´ìš”": ["ã…", "ã…ˆ", "ã„±", "ã„¸"],
    "ìë™ì°¨ê°€ ë‹¬ë ¤ìš”": ["ã…ˆ", "ã„·", "ã…Š", "ã„¹"],
    "ë³„ì´ ë°˜ì§ë°˜ì§ ë¹›ë‚˜ìš”": ["ã…‚", "ã…ˆ", "ã„´", "ã…‰"],
    "ë‚˜ëŠ” ì‚¬ê³¼ë¥¼ ì¢‹ì•„í•´ìš”": ["ã„´", "ã……", "ã„±", "ã…˜", "ã…ˆ"],
    "ì˜¤ëŠ˜ì€ ê¸°ë¶„ì´ ì¢‹ì•„ìš”": ["ã…‡", "ã„´", "ã…ˆ", "ã…—"]
}

st.title("\U0001F5E3ï¸ ì¡°ìŒì¥ì•  ì§„ë‹¨ ì‹œìŠ¤í…œ")

if 'selected_sentence' not in st.session_state:
    st.session_state.selected_sentence = list(sentence_to_file.keys())[0]

selected_sentence = st.selectbox(
    "ì§„ë‹¨í•  ë¬¸ì¥ì„ ì„ íƒí•˜ì„¸ìš”:",
    list(sentence_to_file.keys()),
    index=list(sentence_to_file.keys()).index(st.session_state.selected_sentence)
)

st.session_state.selected_sentence = selected_sentence
phonemes = sentence_analysis[selected_sentence]
st.markdown(f"### \U0001F3AF ë¶„ì„í•  ìŒì†Œ: `{', '.join(phonemes)}`")

file_prefix = sentence_to_file[selected_sentence]
ref_coords_path = os.path.join(PROCESSED_DIR, f"{file_prefix}_coords.txt")
user_video_path = os.path.join(RAW_DIR, "user_video.mp4")
user_coords_path = os.path.join(PROCESSED_DIR, "user_coords.txt")

user_file = st.file_uploader("\U0001F4C5 ì‚¬ìš©ì ì˜ìƒ ì—…ë¡œë“œ (mp4)", type=["mp4"])

if user_file:
    with open(user_video_path, "wb") as f:
        f.write(user_file.read())
    st.video(user_video_path)

    if st.button("\U0001F680 ë¶„ì„ ì‹œì‘"):
        if not os.path.exists(ref_coords_path):
            st.error(f"âŒ ê¸°ì¤€ ì¢Œí‘œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {ref_coords_path}")
            st.stop()

        st.info("ğŸ“ ì‚¬ìš©ì ì˜ìƒ â†’ ì…ëª¨ì–‘ ì¢Œí‘œ ì¶”ì¶œ ì¤‘...")
        extract_mouth_landmarks(user_video_path, user_coords_path)

        timeout = 5
        elapsed = 0
        while (not os.path.exists(user_coords_path) or os.path.getsize(user_coords_path) == 0) and elapsed < timeout:
            time.sleep(0.2)
            elapsed += 0.2

        def load_coords(path):
            coords = []
            with open(path, "r") as f:
                for line in f:
                    try:
                        coords.append(ast.literal_eval(line.strip()))
                    except:
                        continue
            return coords

        user_coords = load_coords(user_coords_path)
        ref_coords = load_coords(ref_coords_path)

        if not user_coords or not ref_coords:
            st.error("ğŸš¨ ì¢Œí‘œ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            st.stop()

        # âœ… ì¢Œí‘œ ì°¨ì´ í†µê³„ í™•ì¸ (í”„ë ˆì„ 0 ê¸°ì¤€)
        try:
            u0 = np.array(user_coords[0])
            r0 = np.array(ref_coords[0])
            cut_len = min(len(u0), len(r0))
            u0 = u0[:cut_len].flatten()
            r0 = r0[:cut_len].flatten()

            diff_vector = np.abs(u0 - r0)
            mean_diff = np.mean(diff_vector)
            max_diff = np.max(diff_vector)
            min_diff = np.min(diff_vector)

            st.markdown("### ğŸ“Š ì²« í”„ë ˆì„ ì¢Œí‘œ ì°¨ì´ ë¶„ì„")
            st.text(f"ì°¨ì´ í‰ê· : {mean_diff:.6f}")
            st.text(f"ì°¨ì´ ìµœëŒ€: {max_diff:.6f}")
            st.text(f"ì°¨ì´ ìµœì†Œ: {min_diff:.6f}")
        except Exception as e:
            st.warning(f"ì¢Œí‘œ ì°¨ì´ ë¶„ì„ ì‹¤íŒ¨: {e}")

        # í”„ë ˆì„ë³„ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        min_len = min(len(user_coords), len(ref_coords))
        warned = False

        for i in range(min_len):
            c1 = user_coords[i]
            c2 = ref_coords[i]

            if len(c1) != len(c2):
                if not warned:
                    st.warning(f"âš ï¸ ì¢Œí‘œ ê°œìˆ˜ ë‹¤ë¦„ (ì˜ˆì‹œ í”„ë ˆì„): ì‚¬ìš©ì {len(c1)} vs ê¸°ì¤€ {len(c2)}")
                    warned = True
                cut_len = min(len(c1), len(c2))
                c1 = c1[:cut_len]
                c2 = c2[:cut_len]

            try:
                c1_np = np.array(c1)
                c2_np = np.array(c2)
                distances = np.linalg.norm(c1_np - c2_np, axis=1)
                avg_dist = np.mean(distances)
                similarity_score = max(0.0, 100 - avg_dist * 300)  # ê°ë„ ì¡°ì • ê°€ëŠ¥
                similarities.append(similarity_score)
            except Exception as e:
                st.warning(f"ì¢Œí‘œ ì°¨ì´ ë¶„ì„ ì‹¤íŒ¨ (í”„ë ˆì„ {i}): {e}")
                continue

        similarity = round(sum(similarities) / len(similarities), 1) if similarities else 0.0

        st.markdown(f"### âœ… ìœ ì‚¬ë„: `{similarity}%`")
        if similarity >= 85:
            st.success("ë°œìŒì´ ë§¤ìš° ì •í™•í•©ë‹ˆë‹¤! ğŸ˜„")
        elif similarity >= 60:
            st.warning("ì¡°ê¸ˆ ë” ì—°ìŠµì´ í•„ìš”í•´ìš”. ğŸ™‚")
        else:
            st.error("ì…ëª¨ì–‘ì´ ë§ì´ ë‹¤ë¥´ë„¤ìš”. ì—°ìŠµì´ í•„ìš”í•´ìš”. ğŸ¤­")
