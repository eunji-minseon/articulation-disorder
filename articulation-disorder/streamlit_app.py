import os
from gtts import gTTS
import base64
import whisper
from difflib import SequenceMatcher
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), "articulation-disorder"))

os.environ["STREAMLIT_WATCHER_TYPE"] = "none"
import time
import numpy as np
import streamlit as st
import ast
from datetime import datetime
import pandas as pd
from video.extract_mouth_landmarks import extract_mouth_landmarks

def text_to_speech(text, filename="output.mp3"):
    tts = gTTS(text=text, lang='ko')
    tts.save(filename)
    return filename

def get_stt_text(video_path):
    model = whisper.load_model("base")
    result = model.transcribe(video_path, language='ko')
    return result["text"]

def compare_texts(ref_text, stt_text):
    ratio = SequenceMatcher(None, ref_text, stt_text).ratio()
    return round(ratio * 100, 1)

def normalize_coordinates(coords):
    #ì¢Œí‘œ ì •ê·œí™”
    if not coords:
        return coords
    
    coords_array = np.array(coords)
    
    # ì¤‘ì‹¬ì  ê³„ì‚° (ì–¼êµ´ ìœ„ì¹˜ ë³´ì •)
    center = np.mean(coords_array, axis=0)
    centered_coords = coords_array - center
    
    # ìŠ¤ì¼€ì¼ ì •ê·œí™” (ì–¼êµ´ í¬ê¸° ë³´ì •)
    scale = np.std(centered_coords)
    if scale > 0:
        normalized_coords = centered_coords / scale
    else:
        normalized_coords = centered_coords
    
    return normalized_coords.tolist()

def calculate_improved_similarity(user_coords, ref_coords):
    #ê±°ë¦¬ ê¸°ë°˜ ê³„ì‚° ìœ ì‚¬ë„
    similarities = []
    min_len = min(len(user_coords), len(ref_coords))

    for i in range(min_len):
        c1 = user_coords[i]
        c2 = ref_coords[i]

        if len(c1) != len(c2):
            cut_len = min(len(c1), len(c2))
            c1 = c1[:cut_len]
            c2 = c2[:cut_len]

        try:
            c1_np = np.array(c1)
            c2_np = np.array(c2)

            if np.allclose(c1_np, c2_np, atol=1e-6):
                print(f"âœ… Frame {i}: ì¢Œí‘œ ì™„ì „ ì¼ì¹˜ (ìœ ì‚¬ë„ 100%)")
            else:
                print(f"âŒ Frame {i}: ì¢Œí‘œ ë‹¤ë¦„")
                print("ì°¨ì´:", np.abs(c1_np - c2_np).max())

            # ì¢Œí‘œê°€ ê±°ì˜ ì™„ì „íˆ ê°™ìœ¼ë©´ ìœ ì‚¬ë„ 100%
            if np.allclose(c1_np, c2_np, atol=1e-6):
                similarity_score = 100.0
            else:
                distances = np.linalg.norm(c1_np - c2_np, axis=1)
                avg_dist = np.mean(distances)
                similarity_score = round(100 * np.exp(-2 * avg_dist), 1)

            similarities.append(similarity_score)

        except Exception as e:
            print(f"Error at frame {i}: {e}")
            continue

    return round(sum(similarities) / len(similarities), 1) if similarities else 0.0

if 'user_id' not in st.session_state:
    st.session_state.user_id = ""

st.sidebar.markdown("## ğŸ” ì‚¬ìš©ì ë¡œê·¸ì¸")
nickname_or_email = st.sidebar.text_input("ë‹‰ë„¤ì„ ë˜ëŠ” ì´ë©”ì¼ì„ ì…ë ¥í•˜ì„¸ìš”.", value=st.session_state.user_id)

if nickname_or_email:
    st.session_state.user_id = nickname_or_email
    st.sidebar.success(f"âœ… {nickname_or_email} ë‹˜ìœ¼ë¡œ ë¡œê·¸ì¸ë¨")

user_id = st.session_state.user_id
if not user_id:
    st.error("ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë‹‰ë„¤ì„ ë˜ëŠ” ì´ë©”ì¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
RAW_DIR = os.path.join(BASE_DIR, "data", "raw")
PROCESSED_DIR = os.path.join(BASE_DIR, "data", "processed")
SCORE_LOG_PATH = os.path.join(BASE_DIR, "data", "user_scores.csv")

os.makedirs("data", exist_ok=True)
os.makedirs(RAW_DIR, exist_ok=True)
os.makedirs(PROCESSED_DIR, exist_ok=True)

sentence_to_file = {
    "ê·¸ëŒ€ë¡œ ë©ˆì¶°ë¼": "normal1",
    "ê¼­ë‘ê°ì‹œ ì¸í˜• í”¼ë…¸í‚¤ì˜¤": "normal2",
    "ì½”ë¼ë¦¬ ì•„ì €ì”¨ëŠ” ì½”ê°€ ì†ì´ë˜": "normal3",
    "ë˜ ë§Œë‚˜ìš” ë½€ë½€ë½€": "normal4",
    "ë°˜ì§ë°˜ì§ ì‘ì€ ë³„": "normal5",
    "ì‚°í† ë¼ í† ë¼ì•¼": "normal6",
    "ì‹œê³„ëŠ” ì•„ì¹¨ë¶€í„° ë˜‘ë”±ë˜‘ë”±": "normal7",
    "ìŒ©ìŒ© ë¶ˆì–´ë„ ê´œì°®ì•„ìš”": "normal8",
    "ì˜ˆìœ ì•„ê¸° ê³°": "normal9",
}

sentence_analysis = {
    "ê·¸ëŒ€ë¡œ ë©ˆì¶°ë¼": ["ã…Š", "ã…"],
    "ê¼­ë‘ê°ì‹œ ì¸í˜• í”¼ë…¸í‚¤ì˜¤": ["ã„²", "ã…", "ã…‹"],
    "ì½”ë¼ë¦¬ ì•„ì €ì”¨ëŠ” ì½”ê°€ ì†ì´ë˜": ["ã…‹", "ã„²", "ã…†"],
    "ë˜ ë§Œë‚˜ìš” ë½€ë½€ë½€": ["ã„¸", "ã…›", "ã…ƒ"],
    "ë°˜ì§ë°˜ì§ ì‘ì€ ë³„": ["ã…‰", "ã…•"],
    "ì‚°í† ë¼ í† ë¼ì•¼": ["ã…Œ", "ã„²", "ã…‘"],
    "ì‹œê³„ëŠ” ì•„ì¹¨ë¶€í„° ë˜‘ë”±ë˜‘ë”±": ["ã„¸", "ã…Š", "ã…–"],
    "ìŒ©ìŒ© ë¶ˆì–´ë„ ê´œì°®ì•„ìš”": ["ã…†", "ã…Š", "ã…™"],
    "ì˜ˆìœ ì•„ê¸° ê³°": ["ã…–", "ã…ƒ"],
}

st.title("\U0001F5E3ï¸ ì¡°ìŒì¥ì•  ì§„ë‹¨ ì‹œìŠ¤í…œ")

selected_sentence = st.selectbox("ì§„ë‹¨í•  ë¬¸ì¥ì„ ì„ íƒí•˜ì„¸ìš”.:", list(sentence_to_file.keys()))
phonemes = sentence_analysis[selected_sentence]
st.markdown(f"### âœ… ìœ ì˜í•  ìŒì†Œ: `{', '.join(phonemes)}`")
st.info("ğŸ‘‰ í•´ë‹¹ ìŒì†Œë“¤ì„ ì§‘ì¤‘í•´ì„œ ë°œìŒí•˜ë©´ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì–´ìš”!")


file_prefix = sentence_to_file[selected_sentence]
ref_coords_path = os.path.join(PROCESSED_DIR, f"{file_prefix}_coords.txt")
user_video_path = os.path.join(RAW_DIR, "user_video.mp4")
def load_coords(path):
    coords = []
    with open(path, "r") as f:
        for line in f:
            try:
                coords.append(ast.literal_eval(line.strip()))
            except:
                continue
    return coords

ref_coords = load_coords(ref_coords_path)

user_file = st.file_uploader("ğŸ“¹ ì‚¬ìš©ì ì˜ìƒ ì—…ë¡œë“œ (mp4, mov)", type=["mp4", "mpeg4", "mov"])

if user_file:
    with open(user_video_path, "wb") as f:
        f.write(user_file.read())
    st.video(user_video_path)

    if st.button("ğŸš€ ë¶„ì„ ì‹œì‘"):
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        user_coords_path = os.path.join(PROCESSED_DIR, f"user_coords_{timestamp}.txt")

        with st.spinner(" ì‚¬ìš©ì ì˜ìƒ â†’ ì…ëª¨ì–‘ ì¢Œí‘œ ì¶”ì¶œ ì¤‘ì…ë‹ˆë‹¤..."):
            extract_mouth_landmarks(user_video_path, user_coords_path)

        user_coords = load_coords(user_coords_path)

        if not user_coords or not ref_coords:
            st.error("ğŸš¨ ì¢Œí‘œ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            st.stop()
        # ì¢Œí‘œ ë¶ˆëŸ¬ì˜¨ í›„ ì •ê·œí™”
        user_coords = load_coords(user_coords_path)
        ref_coords = load_coords(ref_coords_path)
        print("ref shape:", np.array(ref_coords).shape)
        print("user shape:", np.array(user_coords).shape)

        similarity = calculate_improved_similarity(user_coords, ref_coords)

        # ğŸ‘„ ì…ëª¨ì–‘ ìœ ì‚¬ë„
        st.markdown(f"#### âœ“ ì¡°ìŒ ì •í™•ë„: `{similarity}%`")

        # ğŸ§  STT ê¸°ë°˜ ë°œí™” ìœ ì‚¬ë„
        with st.spinner("ğŸ™ï¸ ì‚¬ìš©ìì˜ ì‹¤ì œ ë°œí™” ë‚´ìš©ì„ ì¸ì‹ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                stt_result = get_stt_text(user_video_path)
                text_similarity = compare_texts(selected_sentence, stt_result)
                st.markdown(f"#### âœ“ ë°œí™” ì •í™•ë„: `{text_similarity}%`")

                st.markdown(f"#### âœ“ ì¸ì‹ëœ ìŒì„± ê²°ê³¼: `{stt_result}`")

                with st.expander("ğŸ“Š ì§„ë‹¨ ê²°ê³¼", expanded=True):
                    col1, col2 = st.columns(2)

                    with col1:
                        st.metric("ì¡°ìŒ ì •í™•ë„", f"{similarity}%")

                    with col2:
                        st.metric("ë°œí™” ì •í™•ë„", f"{text_similarity}%")

                    st.markdown("#### ğŸ’¬ ì¢…í•© í”¼ë“œë°±")
                    if similarity >= 70 and text_similarity >= 80:
                        st.success("ë°œìŒê³¼ ë‚´ìš© ëª¨ë‘ ì•„ì£¼ ì •í™•í•©ë‹ˆë‹¤! ğŸ˜")
                    elif similarity >= 50 and text_similarity >= 60:
                        st.warning("ì „ë°˜ì ìœ¼ë¡œ ê´œì°®ì§€ë§Œ, ì¡°ìŒì´ë‚˜ ë°œí™” ì¤‘ ì¼ë¶€ê°€ ë¶€ì¡±í•´ìš”.")
                    else:
                        st.error("ì…ëª¨ì–‘ê³¼ ë°œí™” ëª¨ë‘ ì—°ìŠµì´ í•„ìš”í•´ìš”. ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")
            
            except Exception as e:
                st.error(f"ğŸš¨ STT ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # ë¬¸ì¥ë§Œ ì½ê¸°
        sentence_text = f"{selected_sentence}"
        tts_path = text_to_speech(sentence_text, "sentence_only.mp3")

        with open(tts_path, "rb") as f:
            b64 = base64.b64encode(f.read()).decode()
            st.markdown(
                f"""
                <audio controls autoplay>
                    <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
                </audio>
                """,
                unsafe_allow_html=True,
            )

        if similarity is not None and timestamp is not None:
            st.markdown(f"ğŸ“Œ ìµœê·¼ ì ìˆ˜: {similarity}% ({timestamp})")    

        result_row = pd.DataFrame([{
            "user_id": str(user_id),
            "timestamp": timestamp,
            "sentence": selected_sentence,
            "similarity": similarity
        }])

        if os.path.exists(SCORE_LOG_PATH) and os.path.getsize(SCORE_LOG_PATH) > 0:
            score_df = pd.read_csv(SCORE_LOG_PATH)
            score_df = pd.concat([score_df, result_row], ignore_index=True)
        else:
            score_df = result_row

        score_df.to_csv(SCORE_LOG_PATH, index=False)
        st.success("ğŸ“ˆ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!")

if os.path.exists(SCORE_LOG_PATH) and os.path.getsize(SCORE_LOG_PATH) > 0:
    score_df = pd.read_csv(SCORE_LOG_PATH)
else:
    score_df = pd.DataFrame(columns=["user_id", "timestamp", "sentence", "similarity"])

st.markdown("---")
st.markdown("### ğŸ—‚ï¸ ë‚´ ë¶„ì„ ê¸°ë¡")

user_history = score_df[score_df["user_id"] == user_id] if "user_id" in score_df.columns else pd.DataFrame()
try:
    st.dataframe(user_history.sort_values("timestamp", ascending=False).reset_index(drop=True))
except KeyError:
    st.warning("âŒ 'timestamp' ì—´ì´ ì—†ì–´ì„œ ì •ë ¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.dataframe(user_history)

if st.button("ğŸ—‘ï¸ ê¸°ì¡´ ê¸°ë¡ ì™„ì „ ì‚­ì œ"):
    try:
        os.remove(SCORE_LOG_PATH)
        st.success("âœ… ê¸°ì¡´ ê¸°ë¡ ì‚­ì œ ì™„ë£Œ! ì•±ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    except:
        st.warning("âŒ ì‚­ì œí•  íŒŒì¼ì´ ì—†ê±°ë‚˜ ì´ë¯¸ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")