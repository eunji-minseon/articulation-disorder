# âœ… ì™„ì „íˆ ì •ë¦¬í•œ streamlit_app.py (ì•ˆì •í˜• êµ¬ì¡°)

import os
os.environ["STREAMLIT_WATCHER_TYPE"] = "none"
import time
import numpy as np
import streamlit as st
import ast
from datetime import datetime
import pandas as pd
from video.extract_mouth_landmarks import extract_mouth_landmarks

# ê²½ë¡œ ì„¤ì •
RAW_DIR = "data/raw"
PROCESSED_DIR = "data/processed"
SCORE_LOG_PATH = "data/user_scores.csv"
os.makedirs(RAW_DIR, exist_ok=True)
os.makedirs(PROCESSED_DIR, exist_ok=True)

# ë¬¸ì¥ â†’ ê¸°ì¤€ ì¢Œí‘œ íŒŒì¼ prefix
sentence_to_file = {
    "ê°•ì•„ì§€ê°€ ì§–ê³  ìˆì–´ìš”": "normal1",
    "í† ë¼ê°€ í’€ì„ ë¨¹ì–´ìš”": "normal2",
    "ì½”ë¼ë¦¬ëŠ” ì½”ê°€ ê¸¸ì–´ìš”": "normal3",
    "ì°½ë¬¸ì„ ì—´ì–´ ì£¼ì„¸ìš”": "normal4",
    "ê³ ì–‘ì´ê°€ ì•¼ì˜¹í•´ìš”": "normal5",
    "ë¬´ì§€ê°œê°€ ë–´ì–´ìš”": "normal6",
    "ìë™ì°¨ê°€ ë‹¬ë ¤ìš”": "normal7",
    "ë³„ì´ ë°˜ì§ë°˜ì§ ë¹›ë‚˜ìš”": "normal8",
    "ë‚˜ëŠ” ì‚¬ê³¼ë¥¼ ì¢‹ì•„í•´ìš”": "normal9",
    "ì˜¤ëŠ˜ì€ ê¸°ë¶„ì´ ì¢‹ì•„ìš”": "normal10"
}

# ë¬¸ì¥ â†’ ë¶„ì„ ìŒì†Œ
sentence_analysis = {
    "ê°•ì•„ì§€ê°€ ì§–ê³  ìˆì–´ìš”": ["ã„±", "ã…‡", "ã…ˆ", "ã…†"],
    "í† ë¼ê°€ í’€ì„ ë¨¹ì–´ìš”": ["ã…Œ", "ã„²", "ã…", "ã„¹"],
    "ì½”ë¼ë¦¬ëŠ” ì½”ê°€ ê¸¸ì–´ìš”": ["ã…‹", "ã„²", "ã„¹"],
    "ì°½ë¬¸ì„ ì—´ì–´ ì£¼ì„¸ìš”": ["ã…Š", "ã…", "ã…ˆ", "ã……"],
    "ê³ ì–‘ì´ê°€ ì•¼ì˜¹í•´ìš”": ["ã„±", "ã…‡", "ã…‘", "ã…"],
    "ë¬´ì§€ê°œê°€ ë–´ì–´ìš”": ["ã…", "ã…ˆ", "ã„±", "ã„¸"],
    "ìë™ì°¨ê°€ ë‹¬ë ¤ìš”": ["ã…ˆ", "ã„·", "ã…Š", "ã„¹"],
    "ë³„ì´ ë°˜ì§ë°˜ì§ ë¹›ë‚˜ìš”": ["ã…‚", "ã…ˆ", "ã„´", "ã…‰"],
    "ë‚˜ëŠ” ì‚¬ê³¼ë¥¼ ì¢‹ì•„í•´ìš”": ["ã„´", "ã……", "ã„±", "ã…˜", "ã…ˆ"],
    "ì˜¤ëŠ˜ì€ ê¸°ë¶„ì´ ì¢‹ì•„ìš”": ["ã…‡", "ã„´", "ã…ˆ", "ã…—"]
}

st.title("\U0001F5E3ï¸ ì¡°ìŒì¥ì•  ì§„ë‹¨ ì‹œìŠ¤í…œ")

selected_sentence = st.selectbox("ì§„ë‹¨í•  ë¬¸ì¥ì„ ì„ íƒí•˜ì„¸ìš”:", list(sentence_to_file.keys()))
phonemes = sentence_analysis[selected_sentence]
st.markdown(f"### \U0001F3AF ë¶„ì„í•  ìŒì†Œ: `{', '.join(phonemes)}`")

file_prefix = sentence_to_file[selected_sentence]
ref_coords_path = os.path.join(PROCESSED_DIR, f"{file_prefix}_coords.txt")
user_video_path = os.path.join(RAW_DIR, "user_video.mp4")

@st.cache_data
def load_coords(path):
    coords = []
    with open(path, "r") as f:
        for line in f:
            try:
                coords.append(ast.literal_eval(line.strip()))
            except:
                continue
    return coords

ref_coords = load_coords(ref_coords_path)

user_file = st.file_uploader("\U0001F4C5 ì‚¬ìš©ì ì˜ìƒ ì—…ë¡œë“œ (mp4, mov)", type=["mp4", "mpeg4", "mov"])

if user_file:
    with open(user_video_path, "wb") as f:
        f.write(user_file.read())
    st.video(user_video_path)

    if st.button("\U0001F680 ë¶„ì„ ì‹œì‘"):
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        user_coords_path = os.path.join(PROCESSED_DIR, f"user_coords_{timestamp}.txt")

        st.info("ğŸ“ ì‚¬ìš©ì ì˜ìƒ â†’ ì…ëª¨ì–‘ ì¢Œí‘œ ì¶”ì¶œ ì¤‘...")
        extract_mouth_landmarks(user_video_path, user_coords_path)
        user_coords = load_coords(user_coords_path)

        if not user_coords or not ref_coords:
            st.error("ğŸš¨ ì¢Œí‘œ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            st.stop()

        similarities = []
        min_len = min(len(user_coords), len(ref_coords))
        warned = False

        for i in range(min_len):
            c1 = user_coords[i]
            c2 = ref_coords[i]

            if len(c1) != len(c2):
                if not warned:
                    st.warning(f"âš ï¸ ì¢Œí‘œ ê°œìˆ˜ ë‹¤ë¦„ (ì˜ˆì‹œ í”„ë ˆì„): ì‚¬ìš©ì {len(c1)} vs ê¸°ì¤€ {len(c2)}")
                    warned = True
                cut_len = min(len(c1), len(c2))
                c1 = c1[:cut_len]
                c2 = c2[:cut_len]

            try:
                c1_np = np.array(c1)
                c2_np = np.array(c2)
                distances = np.linalg.norm(c1_np - c2_np, axis=1)
                avg_dist = np.mean(distances)
                similarity_score = round(100 * np.exp(-6 * avg_dist), 1)
                similarities.append(similarity_score)
            except:
                continue

        similarity = round(sum(similarities) / len(similarities), 1) if similarities else 0.0

        st.markdown(f"### âœ… ìœ ì‚¬ë„: `{similarity}%`")
        if similarity >= 85:
            st.success("ë°œìŒì´ ë§¤ìš° ì •í™•í•©ë‹ˆë‹¤! ğŸ˜„")
        elif similarity >= 60:
            st.warning("ì¡°ê¸ˆ ë” ì—°ìŠµì´ í•„ìš”í•´ìš”. ğŸ™‚")
        else:
            st.error("ì…ëª¨ì–‘ì´ ë§ì´ ë‹¤ë¥´ë„¤ìš”. ì—°ìŠµì´ í•„ìš”í•´ìš”. ğŸ¤­")

        # ì ìˆ˜ ì €ì¥
        result_row = pd.DataFrame([{
            "timestamp": timestamp,
            "sentence": selected_sentence,
            "similarity": similarity
        }])

        if os.path.exists(SCORE_LOG_PATH):
            score_df = pd.read_csv(SCORE_LOG_PATH)
            score_df = pd.concat([score_df, result_row], ignore_index=True)
        else:
            score_df = result_row

        score_df.to_csv(SCORE_LOG_PATH, index=False)

# ê¸°ë¡ ì¶œë ¥
if os.path.exists(SCORE_LOG_PATH):
    score_df = pd.read_csv(SCORE_LOG_PATH)
else:
    score_df = pd.DataFrame()

st.markdown("---")
st.markdown("### ğŸ—‚ï¸ ì´ì „ ë¶„ì„ ê¸°ë¡")
st.dataframe(score_df.sort_values("timestamp", ascending=False).reset_index(drop=True))

score_df.to_csv(SCORE_LOG_PATH, index=False)
print(f"âœ… ì ìˆ˜ ì €ì¥ë¨: {SCORE_LOG_PATH}")
